# AI Response Classifier

## Team Members

Hancheng Huang, Yiyang He, Guanghao Huang

## Introduction

#### This project compare the LSTM model and BERT model on classify if a paragraph is generate by AI or human being.

Our proposed solution includes training two types of models: a transformer-based model BERT and a non-transformer-based model LSTM. We will fine-tune each model to classify whether the response is generated from human or AI. By comparing their performance, we will determine which model is more accurate for the process of distinguishing.

## Problem Statement

As AI-generated content becomes more common and many AI-generated contents are misused in many places. What’s more, the lack of tools to effectively distinguish response between human and AI-generated causes a risk to integrity, that makes the situation worse. Also, It effects many areas, such as education and customer service. Our goal is trying to train models to solve these issues with high accuracy.

Proposed Solution: Our proposed solution includes training two types of models: a transformer-based model BERT and a non-transformer-based model LSTM. We will fine-tune each model to classify
whether the response is generated from human or AI. By comparing their performance, we will determine which model is more accurate for the process of distinguishing.

## Pre-requisite

- Python 3.9+
- CUDA Toolkit

Require packages for the two models:

- BERT: [./bert/requirements.txt](bert/requirements.txt)
- LSTM: [./lstm/requirements.txt](lstm/requirements.txt)

## How To Run

### 1. BERT

**STEP 1**: Dependency Installation

```
cd bert/
python -m venv bert_venv
./bert_venv/Script/activate
pip install -r requirements.txt
```

<br>

**STEP 2**: Model Download

Since the model file exceeds the Github's file size limit, please please download the following zip file containing the model (`fine_tuned_bert.rar`) and extract it into the `bert/` folder.

- BERT Model: https://drive.google.com/file/d/1wTPgwMcF6BMfGhS3j3rtgP154l3_GoZp/view?usp=sharing

This model is generated from `AIvsHumanBERT.ipynb`.

<br>

**STEP 3**: Run Prediction

Under the `bert/` forder, there are three files:

- `BERT.py`
- `inputs.txt`
- `outputs.txt`

You can put the text you want to test in the inputs.txt file and run the `BERT.py`. The output will be generated in the outputs.txt. Notice that the application count the input by rows, so you need to seperate your inputs in different lines if you want to test multiple inputs. There are example inputs and outputs in `inputs.txt` and `outputs.txt`.

### 2. LSTM

**STEP 1**: Dependency Installation

```
cd lstm/
python -m venv lstm_venv
./lstm_venv/Script/activate
pip install -r requirements.txt
```

**STEP 2**: Model Download

Similarly, since the model file exceeds the Github's file size limit, please download the following pickle file containing the model (`lstm_model.pkl`) and move it into the **`lstm/model/` folder**.

- LSTM Model: https://drive.google.com/file/d/1qFEmO9w77QGWIBciW1LM1F0FYPBQc2tj/view?usp=sharing

This model is directly generated from `lstm_model.ipynb`. If you want, you may regenerate the model by running this notebook script with `OVERWRITE_MODEL = True` (preferrably with CUDA installed).

**STEP 3**: Run Prediction

> [!NOTE]
> To run the prediction, CUDA must be installed. In our design, the pickle model file can only be deserialized with CUDA. It cannot be mapped to a CPU through PyTorch.

Under the `lstm/` folder, run the notebook script `lstm_prediction.ipynb` for prediction. The existing notebook outputs demostrates the prediction of a sentence in the last code-block. To predict something else, modify the `text` variable in the last code-block and re-run the notebook file.

If you are missing any required files (model or tokenizer files), code-block 2 will output a warning.

## Data

### Data Source:

We will collect a dataset from the website(AI Vs Human Text (kaggle.com)). The dataset contains 480000 unique text responses, with each entry classified as human-generated and AI-generated. The dataset includes a feature labeled “# generated” that indicates whether the response is from human(0) or AI(1). The text block contains complete sentences, represented as string data type, which are generated by humans or AI. <br>
![image](https://github.com/user-attachments/assets/df5bf85d-de60-41ae-aae9-e5d1febd83bf)

### Data Split:

Since fine-tuning doe not require a huge data set for getting an accurate result, We pick 50000 data points for training and evaluation. From the 50000 data points, 20% of them are set to be out test set and the other 80% are set to be our train set.

### Data Preprocessing:

(i) removal of special characters and numbers: remove all the special characters in the text such as commas or parentheses. Numbers were also removed from the text. The removal of such characters did not show meaningful differences in the final result as the model is focused on the tokens (i.e.: words) and not on punctuati on, for example<br>
(ii) convert to lower case: convert the text to lower case to make it completely uniform<br>
(iii) combine the text into a single line: in case an abstract has several paragraphs, it was assured that all the text fits to a single line<br>
(iv) remove extra spaces: delete any extra spaces between words resulti ng from previous data treatment, leaving a single space between words. In additon, we are going to try if Stop Words and reduce each word to its base form can help our
models predict better.<br>

## Model Training

### 1. BERT

![image](https://github.com/user-attachments/assets/2e81ea01-f22a-4c48-a99b-4f763424e096)
![97d28f12c64ac7b0fb971e2fc0f4d21](https://github.com/user-attachments/assets/655c6051-6dfe-44ba-80ed-5f8ff382e9b4)
![b5b071a31b8f5ada45552e08907b4ad](https://github.com/user-attachments/assets/9e560120-1906-48f2-8ffd-4d41445ed51a)
![27babbb4ffa20d81fc1a37d1ab7b05e](https://github.com/user-attachments/assets/02502d3c-60d9-40d1-8bae-cb0008539832)

#### Hyper-parameters

1. `Learning rate`: We start as a bigger value with alpha 0.01 and find out it is too big that make our loss and accuracy bounce frequently in the graph. Therefore, we slowly descrease its value to 1e-7 and find out there is always a huge gap arround the 4000th step. Thereofre, we set 1e-5 in our final result to balance the overfitting.
2. `Epoches`: For the number of epoches to train the model, we tried from 2 to 5. Since we are able to get a fine result, we pick 3 to balance the overfitting.
3. `Weight Decay`: For the weight decay, we start with 0.1 to tune and decrease it slowly. It turns out the weights decay does not influence our model much. Therefore, we choose the 0.01 which has the best performance to avoid overfit.

### 2. LSTM

![image](https://github.com/user-attachments/assets/fb3209d7-476e-4904-b6cf-14ea73ca65c8)
![image](https://github.com/user-attachments/assets/e3131ce0-f58e-4e38-a42c-6c9d81bedc0c)
![image](https://github.com/user-attachments/assets/ebc5174d-d4c1-44d2-b853-d86b4e959ce2)
![image](https://github.com/user-attachments/assets/512e4577-d3c9-45f6-a2e7-16a2c25d7e89)



#### Hyper-parameters

For the LSTM model, we have a few parameters:

1. `embedding_dim`: We set our initial dimensionality of the word embedding to 50 and gradually increased it to 150 (step of 50). We found that the higher the dimension, the more semantic nuances of the words can be captured. To reduce overfitting, we decided to go with 100.
2. `hidden_dim`: We set this value to 256. We tried increasing this value, but it's too computational expensive for our computers to run with no apparent benefits.
3. `output_dim`: We set the output dimention to 2 since we are trying to differentiate between human and AI.
4. `n_layers`: We set this value to 3 intially, but since the model is more than capable of reaching a high degree of accuracy with our dataset even with a small number of layers, we ended up setting it to 2 to reduce computational runtime.
5. `num_epoches`: We set this to the same value as the BERT model for better comparison.
6. `learning_rate`: Following the BERT model, we found that increasing this value will overshoot the optimal solution as it leads to poor convergence in training. Decreasing the value will also drastically increase the training runtime. We ended up with 1e-4.

## Results

### Similarities:

Both models exhibit a similar trend, starting with low accuracy and gradually improving over time. During training, both experience a key stage where accuracy drops significantly and then recovers to higher levels. This fluctuation likely reflects the models adjusting and learning to capture the main features of the task.

### Differences:

BERT demonstrates a clear advantage in the early stages of training, achieving high accuracy even at the beginning. This is due to its robust pretraining, which allows it to start with a strong understanding of the task. On the other hand, LSTM depends more on high-quality datasets and extended training to achieve similar performance levels.
